{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "# Get label list from directory\n",
    "def get_labels(root):\n",
    "    dataset_dir = root\n",
    "    labels = []\n",
    "    for sub_folder in os.listdir(dataset_dir):\n",
    "        sub_folder_files = os.listdir(os.path.join(dataset_dir, sub_folder))\n",
    "        for i, filename in enumerate(sub_folder_files):\n",
    "            labels.append(filename)\n",
    "        break\n",
    "    return labels\n",
    "\n",
    "\n",
    "root = './tourism_destination_2/tourism_destination/'\n",
    "labels_ori = get_labels(root)\n",
    "\n",
    "\n",
    "def create_labels_from_filenames(dataset_path):\n",
    "    # List all files in the dataset directory\n",
    "    file_names = os.listdir(dataset_path)\n",
    "\n",
    "    labels = []\n",
    "    for file_name in file_names:\n",
    "        if file_name.endswith(\".jpg\"):\n",
    "            # Remove the file extension to get the label\n",
    "            label = os.path.splitext(file_name)[0]\n",
    "            labels.append(label)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "dataset_path = \"./tourism_destination_2/testing_image/\"\n",
    "labels = create_labels_from_filenames(dataset_path)\n",
    "\n",
    "# Print the labels\n",
    "# print(labels)\n",
    "\n",
    "\n",
    "def check_word_in_another_word(word, another_word):\n",
    "    if word.lower() in another_word.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "y_labels = []\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels_ori)):\n",
    "        if check_word_in_another_word(labels_ori[j], labels[i]):\n",
    "            # y_labels.append(labels_ori[j])\n",
    "            y_labels.append(j)\n",
    "\n",
    "# Print the filtered labels\n",
    "print(len(y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './tourism_destination_2/tourism_destination/'\n",
    "class_labels = get_labels(root)  # Replace with your own class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./models/model7.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 104ms/step - loss: 0.2102 - accuracy: 0.9636\n",
      "Loss: 0.2102116197347641\n",
      "Accuracy: 0.9636363387107849\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the test dataset\n",
    "\n",
    "test_labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15]\n",
    "\n",
    "\n",
    "test_images_dir = './tourism_destination_2/testing_image/'\n",
    "\n",
    "# List all image files in the directory\n",
    "image_files = os.listdir(test_images_dir)\n",
    "images = []\n",
    "\n",
    "for image_file in image_files:\n",
    "    # Load and preprocess the image\n",
    "    image_path = os.path.join(test_images_dir, image_file)\n",
    "    image = PIL.Image.open(image_path)\n",
    "    image = image.resize((150, 150))  # Adjust the size according to your model's input shape\n",
    "    image = np.array(image) / 255.0  # Normalize the image\n",
    "    # image = np.expand_dims(image, axis=-1)  # Add a batch dimension\n",
    "\n",
    "    images.append(image)\n",
    "\n",
    "# Convert the test_images and test_labels to numpy arrays\n",
    "test_images = np.array(images)\n",
    "test_labels = np.array(test_labels)\n",
    "num_classes = 16  # Replace with the actual number of classes\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "\n",
    "results = model.evaluate(test_images, test_labels)\n",
    "print(\"Loss:\", results[0])\n",
    "print(\"Accuracy:\", results[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 108ms/step\n",
      "Confusion Matrix:\n",
      "[[31  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0 20  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 30  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 12  2  0  0  0  1  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 15  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 28  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 24  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 13  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  2  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 21]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the predicted labels and true labels\n",
    "predicted_labels = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predicted_labels, axis=1)\n",
    "true_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_mat = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9519276485834726\n",
      "Precision: 0.9641800905666904\n",
      "Recall: 0.9481956845238095\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "# Print F1 Score, precision and recall\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negatives: 0\n",
      "True Negatives: 31\n",
      "False Positives: 0\n",
      "True Positives: 20\n"
     ]
    }
   ],
   "source": [
    "true_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Calculate the number of samples\n",
    "num_samples = len(true_labels)\n",
    "\n",
    "# Initialize the counts for each category\n",
    "false_negatives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "true_positives = 0\n",
    "\n",
    "# Calculate the counts for each category\n",
    "for i in range(num_samples):\n",
    "    if predicted_labels[i] == 1 and true_labels[i] == 1:\n",
    "        true_positives += 1\n",
    "    elif predicted_labels[i] == 0 and true_labels[i] == 0:\n",
    "        true_negatives += 1\n",
    "    elif predicted_labels[i] == 1 and true_labels[i] == 0:\n",
    "        false_positives += 1\n",
    "    elif predicted_labels[i] == 0 and true_labels[i] == 1:\n",
    "        false_negatives += 1\n",
    "\n",
    "\n",
    "\n",
    "# Print the counts\n",
    "print(\"False Negatives:\", false_negatives)\n",
    "print(\"True Negatives:\", true_negatives)\n",
    "print(\"False Positives:\", false_positives)\n",
    "print(\"True Positives:\", true_positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
